{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-03-03T21:05:46.119126Z",
     "start_time": "2024-03-03T21:05:41.825826Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "import torch.utils.data as data"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "For replication purposes use the following seed"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f0713a630a1cfd54"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-03T21:05:47.825643Z",
     "start_time": "2024-03-03T21:05:47.820887Z"
    }
   },
   "id": "c4286a736bec04cd",
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "source": [
    "Using GPU acceleration can greatly speed up computations in deep learning, as GPUs are designed to perform many parallel computations simultaneously. The torch.device function allows us to specify whether we want to use the GPU or the CPU for computations, and the code will automatically use the specified device for all operations."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "82764b0a7e397a8f"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3.3%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n",
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n",
      "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n",
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n",
      "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])\n",
    "train_data = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "test_data = datasets.MNIST(root='./data', train=False, download=True, transform=transform)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-03T21:05:52.418997Z",
     "start_time": "2024-03-03T21:05:50.208957Z"
    }
   },
   "id": "ff82c5da4469c513",
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "source": [
    "Data normalization is important in training neural networks because it helps to ensure that the input features are on a similar scale, which can improve the convergence of the optimization algorithm. The values 0.1307 and 0.3081 are the mean and standard deviation of the MNIST dataset. We use these values to normalize the input data so that the mean of the input data is 0 and the standard deviation is 1. "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f5b829ed86d3f258"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "train_loader = data.DataLoader(train_data, batch_size=64, shuffle=True, num_workers=2)\n",
    "test_loader = data.DataLoader(test_data, batch_size=64, shuffle=False, num_workers=2)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-03T21:05:59.262009Z",
     "start_time": "2024-03-03T21:05:59.259135Z"
    }
   },
   "id": "fecc44d4748a216",
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(784, 128)\n",
    "        self.fc2 = nn.Linear(128, 256)\n",
    "        self.fc3 = nn.Linear(256, 128)\n",
    "        self.fc4 = nn.Linear(128, 64)\n",
    "        self.fc5 = nn.Linear(64, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 784)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = F.relu(self.fc4(x))\n",
    "        x = self.fc5(x)\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-03T21:06:05.577783Z",
     "start_time": "2024-03-03T21:06:05.574553Z"
    }
   },
   "id": "18e7fe35022b5568",
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "source": [
    "Input batch of MNIST images has a shape of (64, 1, 28, 28). With 64 being the batch size, 1 being the number of channels (grayscale), and 28x28 being the image dimensions. After the input is reshaped using x.view(-1, 784), the shape of the data is (64, 784). \n",
    "This is because the input data is flattened into a 1D tensor of length 784, which is then passed through the fully connected layers. The output of the final fully connected layer has a shape of (64, 10), which represents the predicted class scores for each image in the batch. The predicted class scores are then used to calculate the loss and perform backpropagation during training."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1d53c1a48247214d"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "model = MLP().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-03T21:08:11.951452Z",
     "start_time": "2024-03-03T21:08:11.947381Z"
    }
   },
   "id": "64f55b12d2b5ac64",
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 training loss: 1.805\n",
      "Epoch 2 training loss: 0.481\n",
      "Epoch 3 training loss: 0.321\n",
      "Epoch 4 training loss: 0.253\n",
      "Epoch 5 training loss: 0.206\n",
      "Epoch 6 training loss: 0.171\n",
      "Epoch 7 training loss: 0.147\n",
      "Epoch 8 training loss: 0.128\n",
      "Epoch 9 training loss: 0.114\n",
      "Epoch 10 training loss: 0.101\n",
      "Epoch 11 training loss: 0.091\n",
      "Epoch 12 training loss: 0.083\n",
      "Epoch 13 training loss: 0.075\n",
      "Epoch 14 training loss: 0.069\n",
      "Epoch 15 training loss: 0.062\n",
      "Epoch 16 training loss: 0.057\n",
      "Epoch 17 training loss: 0.053\n",
      "Epoch 18 training loss: 0.048\n",
      "Epoch 19 training loss: 0.045\n",
      "Epoch 20 training loss: 0.041\n",
      "CPU times: user 29.5 s, sys: 3min 6s, total: 3min 36s\n",
      "Wall time: 43.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for epoch in range(20):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for images, labels in train_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    print('Epoch {} training loss: {:.3f}'.format(epoch+1, running_loss/len(train_loader)))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-03T21:08:57.262076Z",
     "start_time": "2024-03-03T21:08:14.136839Z"
    }
   },
   "id": "c3f8e02a41a44b57",
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "source": [
    "Backpropagation is the process of calculating the gradients of the loss function with respect to the model's parameters, which is used to update the model's parameters during optimization. In the training loop above, backpropagation is performed using the loss.backward() method, which calculates the gradients of the loss function with respect to the model's parameters. The optimizer.zero_grad() method is necessary to reset the gradients of the model's parameters to zero before performing backpropagation. This is because the gradients are accumulated by default, and we want to calculate the gradients for each batch separately, so we need to reset the gradients to zero before each batch."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "527f43a15e81ffcd"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test data: 97.35%\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "test_accuracy = 100 * correct / total\n",
    "print('Accuracy on test data: {:.2f}%'.format(test_accuracy))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-03T21:09:04.492296Z",
     "start_time": "2024-03-03T21:09:03.037489Z"
    }
   },
   "id": "500929a72b56dc27",
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "fdbec1eeb39f1df5"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
